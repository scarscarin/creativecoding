{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1p1Fa2tt7ePXVdsEXAFqXFTSrG2l997ir",
      "authorship_tag": "ABX9TyON2VP3g65YA0lQW8Ozlywh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scarscarin/creativecoding/blob/main/%F0%9F%96%95_how_to_give_google_the_middle_finger_%F0%9F%96%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to give Google the middle finger üñï"
      ],
      "metadata": {
        "id": "4eL-RzVMBqJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to give Google the middle finger, you have many options:\n",
        "- you can go the closest Google HQ and flip the receptionist off\n",
        "- you can open [google.com](https://google.com) and show your finger üñï\n",
        "- you can train a model on a self-made dataset of middle fingers, using Google Colab, Google Mediapipe, and Google Drive!"
      ],
      "metadata": {
        "id": "iIwJUsD-Bt5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëâ We are going for the third, assuming you already made a middle fingers dataset.\n",
        "\n",
        "<hr>"
      ],
      "metadata": {
        "id": "dS4-f0xbJ0hK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 0 (zero). Upload your dataset to Google Drive\n",
        "Upload your middle finger dataset to your [Google Drive](https://drive.google.com).\n",
        "\n",
        "Make a folder ``gestures_dataset`` with the subfolders ``middle_finger`` and ``flip-off-google``.\n",
        "\n",
        "In the ``middle_finger``, you put your middle fingers.\n",
        "\n",
        "In ``flip-off-google`` you place a ``class_1`` empty subfolder, and a ``none`` subfolder.\n",
        "\n",
        "In the ``none`` folder you upload the content of [this folder](https://drive.google.com/drive/folders/1-aLmL1egO5l6UPBsY64T7_rigFKOlyG6?usp=drive_link) üìÅ.\n",
        "\n",
        "It should look like this:\n",
        "\n",
        "<img width=\"200px\" src=\"https://i.ibb.co/C6zfPqR/image.png\">"
      ],
      "metadata": {
        "id": "C5gYx76TEW76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ö†Ô∏è ‚ö†Ô∏è ‚ö†Ô∏è\n",
        "\n",
        "In the next cell, you allow access to your Google Drive. Make sure the addresses of the folders' paths are correctly matching your folders' names."
      ],
      "metadata": {
        "id": "fKSF-bFBEYLI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUCC6gLsBQKr",
        "outputId": "d4aef9b6-d48b-4135-cac1-6efa4cf63898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you read ``Mounted at /content/drive`` after running the cell, you can move on üéâ"
      ],
      "metadata": {
        "id": "mSd4fnw-ErVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 (one). Crop images"
      ],
      "metadata": {
        "id": "DR4H6bc-E1VX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to train a model on a dataset of middle finger images, we need to clean up each image by cropping the finger, and the finger only.\n",
        "\n",
        "<img width=\"50%\"  src=\"https://i.ibb.co/YWWGLbW/Untitled.png\">\n"
      ],
      "metadata": {
        "id": "CMdBFgGjK_jV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do this, we need to install a few extra tools.\n",
        "\n",
        "[Mediapipe](https://ai.google.dev/edge/mediapipe/solutions/guide) is a machine learning solution by Google. It will help our code *recognise* the hands in each image, and crop them accordingly."
      ],
      "metadata": {
        "id": "mkR9aTX5R5Wz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "install Mediapipe using the shell-script ``!pip install``"
      ],
      "metadata": {
        "id": "wE-T5QowSS1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGrwtCIdhsX5",
        "outputId": "9db6ecca-586e-4c34-d3e6-da2bc8959406",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.18)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.8.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also use ``cv2`` for image processing, ``os`` for file handling, and ``numpy`` for complex numerical operations. These ones are normally already installed in the Google Colab environment you are using.\n",
        "\n",
        "``import`` all the libraries in python.\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "aWvyLZgwSaEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "aqpR-BS8eZa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the ``mediapipe`` solutions for your code by creating shortnames for them, like ``mp_hands``, ``hands``, and ``mp_drawing``."
      ],
      "metadata": {
        "id": "VjJjyquHSuZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2)"
      ],
      "metadata": {
        "id": "XWgYoA18efD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to point our code to the correct Google Drive folders. Our initial dataset, and the folder where we save our cropped images"
      ],
      "metadata": {
        "id": "_8VcrCEaeo7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_folder = \"/content/drive/MyDrive/flip-off-google/finger\"\n",
        "output_folder = \"/content/drive/MyDrive/flip-off-google/gestures_dataset/class_1\"\n",
        "\n",
        "# Ensure output folder exists\n",
        "os.makedirs(output_folder, exist_ok=True)"
      ],
      "metadata": {
        "id": "v7-v0O7UemG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are preparing the images for training, we also want to make them lighter. That way, the model will <i>digest</i> them faster and more efficiently.\n",
        "\n",
        "<img width=\"50%\" src=\"https://i.ibb.co/6RnJwmK/Untitled.png\">\n",
        "\n",
        "Below we can determine a target size."
      ],
      "metadata": {
        "id": "SO7UeAFGe4Zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_size = (224, 224)"
      ],
      "metadata": {
        "id": "8s79Xga3e1bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we ``def``ine our function ``add_margin_dynamic()``.\n",
        "\n",
        "It is a function that, before cropping, adds a margin. So that our hand is visible in full!"
      ],
      "metadata": {
        "id": "CRs-8Vu9TObY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_margin_dynamic(x_min, y_min, x_max, y_max, img_shape, margin):\n",
        "    h, w = img_shape[:2]\n",
        "    x_margin = int((x_max - x_min) * margin)\n",
        "    y_margin = int((y_max - y_min) * margin)\n",
        "    x_min = max(0, x_min - x_margin)\n",
        "    x_max = min(w, x_max + x_margin)\n",
        "    y_min = max(0, y_min - y_margin)\n",
        "    y_max = min(h, y_max + y_margin)\n",
        "    return x_min, y_min, x_max, y_max"
      ],
      "metadata": {
        "id": "SEWwDPJRFQFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we *loop* through our ‚û∞ image folder, ‚û∞ read each image, ‚û∞ take the hand position, and ‚û∞ crop + resize.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qvpf1JBKf_Am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for img_name in os.listdir(input_folder):\n",
        "    img_path = os.path.join(input_folder, img_name)\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    if img is None:\n",
        "        print(f\"Skipping non-image file: {img_name}\")\n",
        "        continue\n",
        "\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    results = hands.process(img_rgb)\n",
        "\n",
        "    if results.multi_hand_landmarks:\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "            h, w, _ = img.shape\n",
        "            x_min = int(min([lm.x for lm in hand_landmarks.landmark]) * w)\n",
        "            x_max = int(max([lm.x for lm in hand_landmarks.landmark]) * w)\n",
        "            y_min = int(min([lm.y for lm in hand_landmarks.landmark]) * h)\n",
        "            y_max = int(max([lm.y for lm in hand_landmarks.landmark]) * h)\n",
        "\n",
        "            margin = 0.3\n",
        "            x_min, y_min, x_max, y_max = add_margin_dynamic(x_min, y_min, x_max, y_max, img.shape, margin)\n",
        "\n",
        "            cropped_img = img[y_min:y_max, x_min:x_max]\n",
        "\n",
        "            if cropped_img.size > 0:\n",
        "                cropped_img_resized = cv2.resize(cropped_img, target_size)\n",
        "                output_name = os.path.splitext(img_name)[0] + \".jpg\"\n",
        "                output_path = os.path.join(output_folder, output_name)\n",
        "                cv2.imwrite(output_path, cropped_img_resized)\n",
        "                print(f\"Processed and saved: {output_name}\")\n",
        "    else:\n",
        "        print(f\"No hand detected in: {img_name}\")\n",
        "\n",
        "print(\"Processing complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN0AG1WTf8GN",
        "outputId": "0bcb9616-191d-48bb-9cfe-9e7d62e97a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved: 11.jpg\n",
            "Processed and saved: 39.jpg\n",
            "Processed and saved: 35.jpg\n",
            "Processed and saved: 34.jpg\n",
            "Processed and saved: 32.jpg\n",
            "Processed and saved: 33.jpg\n",
            "Processed and saved: 30.jpg\n",
            "Processed and saved: 31.jpg\n",
            "Processed and saved: 27.jpg\n",
            "Processed and saved: 28.jpg\n",
            "Processed and saved: 29.jpg\n",
            "Processed and saved: 3.jpg\n",
            "Processed and saved: 26.jpg\n",
            "Processed and saved: 24.jpg\n",
            "Processed and saved: 18.jpg\n",
            "Processed and saved: 21.jpg\n",
            "Processed and saved: 22.jpg\n",
            "Processed and saved: 2.jpg\n",
            "Processed and saved: 19.jpg\n",
            "Processed and saved: 25.jpg\n",
            "Processed and saved: 23.jpg\n",
            "Processed and saved: 20.jpg\n",
            "Processed and saved: 17.jpg\n",
            "Processed and saved: 16.jpg\n",
            "Processed and saved: 1.jpg\n",
            "Processed and saved: 10.jpg\n",
            "Processed and saved: 0.jpg\n",
            "Processed and saved: 12.jpg\n",
            "Processed and saved: 15.jpg\n",
            "Processed and saved: 13.jpg\n",
            "Processed and saved: 14.jpg\n",
            "Processed and saved: 70.jpg\n",
            "Processed and saved: 69.jpg\n",
            "Processed and saved: 7.jpg\n",
            "Processed and saved: 67.jpg\n",
            "Processed and saved: 68.jpg\n",
            "Processed and saved: 65.jpg\n",
            "Processed and saved: 64.jpg\n",
            "Processed and saved: 63.jpg\n",
            "Processed and saved: 66.jpg\n",
            "Processed and saved: 62.jpg\n",
            "Processed and saved: 61.jpg\n",
            "Processed and saved: 6.jpg\n",
            "Processed and saved: 60.jpg\n",
            "Processed and saved: 59.jpg\n",
            "Processed and saved: 58.jpg\n",
            "Processed and saved: 55.jpg\n",
            "Processed and saved: 56.jpg\n",
            "Processed and saved: 54.jpg\n",
            "Processed and saved: 42.jpg\n",
            "Processed and saved: 43.jpg\n",
            "Processed and saved: 4.jpg\n",
            "Processed and saved: 40.jpg\n",
            "Processed and saved: 38.jpg\n",
            "Processed and saved: 36.jpg\n",
            "Processed and saved: 37.jpg\n",
            "Processed and saved: 41.jpg\n",
            "Processed and saved: 57.jpg\n",
            "Processed and saved: 47.jpg\n",
            "Processed and saved: 45.jpg\n",
            "Processed and saved: 49.jpg\n",
            "Processed and saved: 5.jpg\n",
            "Processed and saved: 46.jpg\n",
            "Processed and saved: 50.jpg\n",
            "Processed and saved: 52.jpg\n",
            "Processed and saved: 51.jpg\n",
            "Processed and saved: 48.jpg\n",
            "Processed and saved: 44.jpg\n",
            "Processed and saved: 53.jpg\n",
            "Processed and saved: 94.jpg\n",
            "Processed and saved: 95.jpg\n",
            "Processed and saved: 90.jpg\n",
            "Processed and saved: 92.jpg\n",
            "Processed and saved: 91.jpg\n",
            "Processed and saved: 93.jpg\n",
            "Processed and saved: 9.jpg\n",
            "Processed and saved: 89.jpg\n",
            "Processed and saved: 85.jpg\n",
            "Processed and saved: 88.jpg\n",
            "Processed and saved: 86.jpg\n",
            "Processed and saved: 87.jpg\n",
            "Processed and saved: 82.jpg\n",
            "Processed and saved: 81.jpg\n",
            "Processed and saved: 83.jpg\n",
            "Processed and saved: 78.jpg\n",
            "Processed and saved: 74.jpg\n",
            "Processed and saved: 77.jpg\n",
            "Processed and saved: 72.jpg\n",
            "Processed and saved: 76.jpg\n",
            "Processed and saved: 73.jpg\n",
            "Processed and saved: 79.jpg\n",
            "Processed and saved: 80.jpg\n",
            "Processed and saved: 71.jpg\n",
            "Processed and saved: 75.jpg\n",
            "Processed and saved: 8.jpg\n",
            "Processed and saved: 84.jpg\n",
            "Processing complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 (two). Train a Gesture Recognition model."
      ],
      "metadata": {
        "id": "4BIEn8IVV7pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe-model-maker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l73M6Fswlu3y",
        "outputId": "be9feb46-1b1f-4f2b-dccd-72713f058874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe-model-maker in /usr/local/lib/python3.10/dist-packages (0.2.1.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (1.4.0)\n",
            "Requirement already satisfied: mediapipe>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (0.10.18)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (4.10.0.84)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.10 in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (0.23.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (4.9.7)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (0.16.1)\n",
            "Requirement already satisfied: tensorflow-model-optimization<0.8.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (0.7.5)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (2.15.0)\n",
            "Requirement already satisfied: tf-models-official<2.16.0,>=2.13.2 in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (2.15.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (3.8.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (4.25.5)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.6.3)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (2.15.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization<0.8.0->mediapipe-model-maker) (0.1.8)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (3.0.11)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (11.0.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.151.0)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (4.2.1)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.6.17)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (4.1.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (4.10.0.84)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (9.0.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.0.8)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (6.0.2)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.13.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.2.2)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.1.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub->mediapipe-model-maker) (2.15.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons->mediapipe-model-maker) (2.13.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (8.1.7)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (2.3)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (17.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (2.32.3)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (1.13.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (4.66.6)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (0.5.1)\n",
            "Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->mediapipe-model-maker) (1.10.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.45.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->mediapipe-model-maker) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->mediapipe-model-maker) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->mediapipe-model-maker) (3.21.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (6.2.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->mediapipe-model-maker) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->mediapipe-model-maker) (3.10)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe>=0.10.0->mediapipe-model-maker) (1.17.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (3.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.4.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (4.9)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (3.0.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (5.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.5.2)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets->mediapipe-model-maker) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->mediapipe-model-maker) (1.66.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe>=0.10.0->mediapipe-model-maker) (2.22)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.25.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (5.5.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.3.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.0.2)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import gesture_recognizer\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ahNbEDiagtDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/MyDrive/flip-off-google/gestures_dataset\""
      ],
      "metadata": {
        "id": "3mNeGH_Wgzwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_path)\n",
        "labels = []\n",
        "for i in os.listdir(dataset_path):\n",
        "  if os.path.isdir(os.path.join(dataset_path, i)):\n",
        "    labels.append(i)\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LitdaIbRcdEI",
        "outputId": "59a7399c-46de-4e2d-be94-6730c0b4a6b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/flip-off-google/gestures_dataset\n",
            "['none', 'class_1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EXAMPLES = 5\n",
        "\n",
        "for label in labels:\n",
        "  label_dir = os.path.join(dataset_path, label)\n",
        "  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n",
        "  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n",
        "  for i in range(NUM_EXAMPLES):\n",
        "    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n",
        "    axs[i].get_xaxis().set_visible(False)\n",
        "    axs[i].get_yaxis().set_visible(False)\n",
        "  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6H98HXz4cnOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = gesture_recognizer.Dataset.from_folder(\n",
        "    dirname=dataset_path,\n",
        "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
        ")\n",
        "train_data, rest_data = data.split(0.8)\n",
        "validation_data, test_data = rest_data.split(0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoBKgXJKcrNz",
        "outputId": "12f72939-d324-4ca9-e26b-f3662d0d8c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing files at /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n",
            "Using existing files at /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n",
            "Using existing files at /tmp/model_maker/gesture_recognizer/gesture_embedder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = gesture_recognizer.HParams(export_dir=\"exported_model\")\n",
        "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n",
        "model = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooVFhYWXe8fQ",
        "outputId": "eba01d66-de41-40ad-d51f-e23ee1ac4892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " hand_embedding (InputLayer  [(None, 128)]             0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 128)               512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_2 (ReLU)              (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " custom_gesture_recognizer_  (None, 2)                 258       \n",
            " out (Dense)                                                     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 770 (3.01 KB)\n",
            "Trainable params: 514 (2.01 KB)\n",
            "Non-trainable params: 256 (1.00 KB)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Resuming from exported_model/epoch_models/model-0010\n",
            "Epoch 1/10\n",
            "59/59 [==============================] - 2s 16ms/step - loss: 0.1196 - categorical_accuracy: 0.8136 - val_loss: 0.0894 - val_categorical_accuracy: 0.8667 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "59/59 [==============================] - 1s 22ms/step - loss: 0.0789 - categorical_accuracy: 0.9068 - val_loss: 0.0769 - val_categorical_accuracy: 0.9333 - lr: 9.9000e-04\n",
            "Epoch 3/10\n",
            "59/59 [==============================] - 1s 21ms/step - loss: 0.0781 - categorical_accuracy: 0.8644 - val_loss: 0.0699 - val_categorical_accuracy: 0.8667 - lr: 9.8010e-04\n",
            "Epoch 4/10\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.0685 - categorical_accuracy: 0.8814 - val_loss: 0.0676 - val_categorical_accuracy: 0.8000 - lr: 9.7030e-04\n",
            "Epoch 5/10\n",
            "59/59 [==============================] - 1s 12ms/step - loss: 0.0652 - categorical_accuracy: 0.8814 - val_loss: 0.0610 - val_categorical_accuracy: 0.8667 - lr: 9.6060e-04\n",
            "Epoch 6/10\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.0593 - categorical_accuracy: 0.9407 - val_loss: 0.0551 - val_categorical_accuracy: 0.9333 - lr: 9.5099e-04\n",
            "Epoch 7/10\n",
            "59/59 [==============================] - 1s 12ms/step - loss: 0.0562 - categorical_accuracy: 0.9237 - val_loss: 0.0511 - val_categorical_accuracy: 0.9333 - lr: 9.4148e-04\n",
            "Epoch 8/10\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.0680 - categorical_accuracy: 0.9153 - val_loss: 0.0497 - val_categorical_accuracy: 0.9333 - lr: 9.3207e-04\n",
            "Epoch 9/10\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.0639 - categorical_accuracy: 0.9068 - val_loss: 0.0453 - val_categorical_accuracy: 0.9333 - lr: 9.2274e-04\n",
            "Epoch 10/10\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.0447 - categorical_accuracy: 0.9407 - val_loss: 0.0453 - val_categorical_accuracy: 0.9333 - lr: 9.1352e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(test_data, batch_size=1)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1sqmstqe-ZF",
        "outputId": "cef1bf2f-6ddb-4a63-9f9e-0e3ece9e4979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 1s 3ms/step - loss: 0.1218 - categorical_accuracy: 0.8571\n",
            "Test loss:0.12184027582406998, Test accuracy:0.8571428656578064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.export_model()\n",
        "!ls exported_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wo0uJ-9gFoZ",
        "outputId": "46c20e5d-b47b-41d7-a848-136b008edbfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing files at /tmp/model_maker/gesture_recognizer/gesture_embedder.tflite\n",
            "Using existing files at /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n",
            "Using existing files at /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n",
            "Using existing files at /tmp/model_maker/gesture_recognizer/canned_gesture_classifier.tflite\n",
            "best_model_weights.data-00000-of-00001\tcheckpoint    gesture_recognizer.task  metadata.json\n",
            "best_model_weights.index\t\tepoch_models  logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('exported_model/gesture_recognizer.task')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "RPAfbaesgSW9",
        "outputId": "3c267489-86e2-4cfd-c158-c62b44fd8e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a3a4dda6-81bd-411c-93c7-548942e3a6a3\", \"gesture_recognizer.task\", 8459847)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}